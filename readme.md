Company Profile Matching API
Overview
This project is a scalable pipeline and API for extracting, analyzing, and searching company profile data from a predefined list of websites. The pipeline scrapes company data (phone numbers, social links, addresses), merges it with a reference dataset, analyzes extraction quality, and exposes a REST API to search and retrieve the best-matching company profile by name, website, phone, and/or Facebook profile.

Tech Stack: Python, FastAPI, BeautifulSoup, Requests, Pandas, phonenumbers, ThreadPoolExecutor, tqdm

Features
Massively parallel, robust web scraping for fast extraction of phone numbers, social links, and addresses from hundreds of company websites.

Data analysis (coverage and fill rates) for quality assessment.

Merge and store company metadata and scraped info into a unified JSON database.

Fuzzy company search REST API: find best-matching company profile by name, domain, phone, and/or Facebook.

Batch testing script to measure match rates and accuracy.

Directory Structure
graphql
Copy
Edit
.
├── main.py                  # Main extraction, analysis, merging, and API logic
├── test_batch.py            # Batch test script for API evaluation
├── CSVs/
│   ├── sample-websites.csv                # List of websites to crawl
│   ├── sample-websites-company-names.csv  # Reference company names/metadata
│   └── API-input-sample.csv               # Input sample for batch API evaluation
├── company_profiles.json    # Output data (autogenerated)
└── README.md                # This file
1. Data Extraction
1.1 Scraping
Extracts for each website:

Phone numbers: using both visible text patterns and <a href="tel:"> links, normalized to E.164 format.

Social media links: Facebook, Twitter, LinkedIn, Instagram (regex + HTML anchors).

Addresses: Looks for content in <footer>, <address>, and divs/spans with address/location in class/id.

Parallelized with ThreadPoolExecutor for speed (32+ workers, configurable).

User-agent rotation and multiple URL formats (with/without www., http/https).

1.2 Data Analysis
Reports coverage (websites successfully crawled) and fill rates for each data type.

Example output:

yaml
Copy
Edit
COVERAGE: 73% | PHONES: 40% | SOCIALS: 31% | ADDRESSES: 14%
1.3 Scaling
Processes ~1000 websites in under 10 minutes on modern hardware (parallelization, timeouts).

Chunked HTML parsing for efficiency and resilience.

2. Data Retrieval
2.1 Storing
Merges scraped data with company names/metadata (sample-websites-company-names.csv).

Outputs a unified JSON (company_profiles.json) for fast lookup.

2.2 Querying (API)
FastAPI app exposes /company/search endpoint.

Accepts query params: name, domain, phone, facebook.

Uses a fuzzy matching algorithm (based on sequence similarity) to score and select the best matching company profile.

Example request:

pgsql
Copy
Edit
GET /company/search?name=Acorn%20Law%20P.C.&domain=acornlawpc.com
Example response:

json
Copy
Edit
{
  "domain": "acornlawpc.com",
  "company_commercial_name": "Acorn Law P.C.",
  "phones": ["+15551234567"],
  "social_links": {"facebook": ["https://facebook.com/acornlaw"]},
  "address": "123 Acorn St, City, State",
  ...
  "match_score": 7.04
}
3. Batch Testing & Match Accuracy
Provided test_batch.py runs the API over API-input-sample.csv and reports:

Match scores and first-5-letter name-match accuracy.

Saves results to batch_results.csv for review.

Example output:

sql
Copy
Edit
Average match score: 4.32
Name-match accuracy (first 5 letters): 50.0 %
Results saved to batch_results.csv
Usage
Setup
Install requirements:

bash
Copy
Edit
pip install -r requirements.txt
(requirements.txt should include: fastapi, uvicorn, requests, beautifulsoup4, pandas, tqdm, phonenumbers)

Run data extraction and prepare the API:

bash
Copy
Edit
python main.py
This will scrape all websites, analyze coverage, merge data, and produce company_profiles.json.

Start the API server:

bash
Copy
Edit
uvicorn main:app --reload
(If company_profiles.json is missing, main.py will create it on launch.)

Query the API (example):

perl
Copy
Edit
http://127.0.0.1:8000/company/search?name=Acorn%20Law%20P.C.&domain=acornlawpc.com
Batch evaluate matching quality:

bash
Copy
Edit
python test_batch.py
See batch_results.csv for details.

File Descriptions
main.py — Complete scraper, merger, analyzer, and API.

test_batch.py — Loads the input sample CSV, calls the API for each entry, saves & prints results.

company_profiles.json — Data file (auto-generated).

CSVs/sample-websites.csv — Websites to crawl.

CSVs/sample-websites-company-names.csv — Company reference data.

CSVs/API-input-sample.csv — Sample queries for batch testing.

How it Works (Code and Logic)
Scrapes each website in parallel for phone numbers, social media, and address (with robust extraction methods and fallbacks).

All data is normalized and merged by domain.

Querying is fuzzy: no need for exact input; the algorithm matches on company names, domains, phones, and Facebook links with weighted similarity.

Reports accuracy using both average score and “first-5-letter” name match heuristic.

Measuring Accuracy
Coverage = How many sites returned valid data.

Fill rate = % of phone/social/address present in the scraped set.

Match accuracy = % of test queries whose output matches the input company (using first-5-letter name match and score).

Rationale
Parallel scraping for speed/scalability.

Flexible, fault-tolerant matching (robust to incomplete, partial, or fuzzy input).

Standard formats (E.164 for phones, lowercased clean domains, etc.).

FastAPI for speed, developer-friendliness, and simple testing.

Batch test for transparent, automated measurement of real match accuracy.

Creative/Advanced Aspects
The pipeline can be trivially scaled up for thousands or millions of records.

Matching algorithm can be further improved using fuzzywuzzy, Jaccard, or hybrid string/token similarity.

All steps are modular: you can plug in Elasticsearch, persist to a DB, or adapt for live crawling.

Running Notes
You can improve coverage and accuracy by tweaking timeout, user-agents, or extraction patterns.

Not every site will expose all data; some sites are minimal, some block bots, etc. The algorithm is designed to gracefully handle these.